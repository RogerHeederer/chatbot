{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train-kogpt2-by-total.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOaYxtBs9BW2VoJVYdKXp+S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sBDFZUyO9-n8"},"source":["##Wellness 심리 상담 데이터 + ChatBot 데이터를 활용한 KOGPT2 학습##"]},{"cell_type":"markdown","metadata":{"id":"HfTS7dvj-MIN"},"source":["1. 구글 드라이브 연동\n","\n","* 모델 파일, 훈련 데이터가 저장 된 구글 드라이브 디렉토리와 Colab을 연동시킨다\n","\n","* 런타임 유형을 GPU로 변경시킨다.(학습 시간이 꽤 걸림)"]},{"cell_type":"code","metadata":{"id":"b38lLqN37RT-","executionInfo":{"status":"ok","timestamp":1603946982984,"user_tz":-540,"elapsed":987,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"ba09526b-fdc9-4cf8-b005-1617cc930322","colab":{"base_uri":"https://localhost:8080/"}},"source":["#GPU 연동 확인\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Oct 29 04:49:42 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9DCikwlh-hxk","executionInfo":{"status":"ok","timestamp":1603947005044,"user_tz":-540,"elapsed":21946,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"52a9b976-7252-402e-bbc8-480ca2f0cfb7","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pALSahlq-yMc","executionInfo":{"status":"ok","timestamp":1603947011238,"user_tz":-540,"elapsed":1408,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"19779faa-2350-402f-bc49-c0cc7696c6be","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls drive/'My Drive'/'RogerHeederer'/'ChatBot'/'KoGPT2_Wellness'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["checkpoint  dataloader\tmodel_play  requirements.txt\n","data\t    model\tpreprocess  train\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jJmfNIad_YEb"},"source":["필요한 패키지들 설치"]},{"cell_type":"code","metadata":{"id":"m9lUMwa3_BkO","executionInfo":{"status":"ok","timestamp":1603947028056,"user_tz":-540,"elapsed":14836,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"af799b6e-f6d5-44ac-93d6-4716c4120b7d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install -r drive/'My Drive'/'RogerHeederer'/'ChatBot'/'KoGPT2_Wellness'/requirements.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting kogpt2-transformers\n","  Downloading https://files.pythonhosted.org/packages/c1/be/3ea089fa2db6c4a9ac0557ac3830593944e255dab8a595f7e42f1a35e1c6/kogpt2_transformers-0.3.1-py3-none-any.whl\n","Collecting kobert-transformers\n","  Downloading https://files.pythonhosted.org/packages/f3/6d/f4e21513c1f26cacd68c144a428ccaa90dd92d85985e878976ebbaf06624/kobert_transformers-0.4.1-py3-none-any.whl\n","Collecting transformers==3.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 10.5MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 4)) (1.6.0+cu101)\n","Collecting kss\n","  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n","Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (1.1.2)\n","Collecting flask_restful\n","  Downloading https://files.pythonhosted.org/packages/e9/83/d0d33c971de2d38e54b0037136c8b8d20b9c83d308bc6c220a25162755fd/Flask_RESTful-0.3.8-py2.py3-none-any.whl\n","Collecting tokenizers>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 17.6MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 34.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (1.18.5)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 45.3MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 4)) (0.16.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (2.11.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (7.1.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask_restful->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 7)) (2018.9)\n","Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from flask_restful->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 7)) (1.15.0)\n","Collecting aniso8601>=0.82\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (0.17.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->-r drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/requirements.txt (line 6)) (1.1.1)\n","Building wheels for collected packages: kss, sacremoses\n","  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251581 sha256=334456b59d72b90f00c054234afc07f2f1dbf8360901e11be52ec17376fb5303\n","  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=e24cc6f3ca82d5bd553255692061fbf02aebe25806170ad33a24503165a62a90\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built kss sacremoses\n","\u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.9.3 which is incompatible.\u001b[0m\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers, kogpt2-transformers, kobert-transformers, kss, aniso8601, flask-restful\n","Successfully installed aniso8601-8.0.0 flask-restful-0.3.8 kobert-transformers-0.4.1 kogpt2-transformers-0.3.1 kss-1.3.1 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.3 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ty8VYsF4_k2W"},"source":["트랜스포머의 토크나이져 버젼이 맞지 않는다고 하여, 기 설치된 토크나이저를 제거 한 후 새로운 토크나이져 설치"]},{"cell_type":"code","metadata":{"id":"ZOG4jk0R_g_b","executionInfo":{"status":"ok","timestamp":1603947039790,"user_tz":-540,"elapsed":8007,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"9cb74599-9c7c-4a0f-c0f2-9682cbcde78b","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip uninstall tokenizers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Uninstalling tokenizers-0.9.3:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/tokenizers-0.9.3.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/tokenizers/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tokenizers-0.9.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-LHOGi6T_sLn","executionInfo":{"status":"ok","timestamp":1603947047132,"user_tz":-540,"elapsed":4123,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"c19ef18b-e9dc-4e63-c200-47c80224307f","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install tokenizers==0.8.1.rc1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 8.6MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.8.1rc1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ga4a_L70_4QR"},"source":["**자 이제 본격적으로 KoGPT2를 정신상담 데이터 셋으로 훈련시켜보자**"]},{"cell_type":"code","metadata":{"id":"La3Dg0m6_xJv","executionInfo":{"status":"ok","timestamp":1603947048863,"user_tz":-540,"elapsed":701,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["# 훈련 데이터, 데이터 로더 등을 불러오기 위해 경로 설정\n","import sys\n","sys.path.append('drive/My Drive/RogerHeederer/ChatBot')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXXOt6FvAIdM","executionInfo":{"status":"ok","timestamp":1603947057103,"user_tz":-540,"elapsed":8257,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["import os\n","import numpy as np\n","from tqdm import tqdm #파이썬 진행표시바 라이브러리\n","\n","import torch\n","from torch.utils.data import dataloader\n","\n","#--내가 구성한 폴더 속의 데이터 로더를 불러오는 것임--#\n","#--원래 코드에는 DialogLM 이라는 폴더로 구성되어 있었는데--#\n","#--초급자 입장에서는 공식 라이브러리로 간주하여, import가 안되서 꽤 헤맸던 요소 --#\n","\n","#KoGPT2_Wellness폴더의 dataloader폴더의 total_data_load.py에서 import 클래스 이름\n","from KoGPT2_Wellness.dataloader.total_data_load import TotalAutoRegressiveDataset\n","#KoGPT2_Wellness폴더의 model 폴더의 kogpt2.py에서 import 클래스 이름\n","from KoGPT2_Wellness.model.kogpt2 import DialogKoGPT2"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKqm2a9bCj8n","executionInfo":{"status":"ok","timestamp":1603947060776,"user_tz":-540,"elapsed":1175,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"4081a5c4-3ff4-4c91-fdcd-0ebf9cbbb035","colab":{"base_uri":"https://localhost:8080/"}},"source":["torch.cuda.is_available()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"E_nO5JvTENXc"},"source":["##KoGPT2 Training by Total dataset(Wellness+chatbot)##"]},{"cell_type":"markdown","metadata":{"id":"Uish45LhtWER"},"source":["GPT-2 is auto-regressive but Bert is not\n"," - After each token is produced, that token is added to the sequence of inputs\n","\n","**AutoRegressive 구조**\n","\n","각각의 토큰이 생성이 되면, 그 생성된 토큰이 그 다음 토큰을 생성시키기 위한 인풋으로 사용되는 구조"]},{"cell_type":"code","metadata":{"id":"7p2rC6uuD4EB","executionInfo":{"status":"ok","timestamp":1603947312000,"user_tz":-540,"elapsed":1122,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["root_path = \"drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness\"\n","data_path = f\"{root_path}/data/total_train.txt\"\n","checkpoint_path = f\"{root_path}/checkpoint\"\n","save_ckpt_path = f\"{checkpoint_path}/total_autoregressive.pth\""],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"YoASjASPEz2F","executionInfo":{"status":"ok","timestamp":1603947323950,"user_tz":-540,"elapsed":12690,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"fac9bfbe-a883-4c21-bf4a-4df864dfda5d","colab":{"base_uri":"https://localhost:8080/"}},"source":["n_epoch = 5    # Num of Epoch/ 기존 오리지널 코드는 epoch 5 늘려봄\n","batch_size = 2 # 배치 사이즈\n","ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(ctx)\n","save_step = 100 # 학습 저장 주기\n","learninig_rate = 5e-5 # Learning Rate\n","\n","dataset = TotalAutoRegressiveDataset(data_path) # total_train.txt를 데이터 셋으로 구성 후 로딩\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = DialogKoGPT2() #모델 이니셜라이즈\n","model.to(device)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DialogKoGPT2(\n","  (kogpt2): GPT2LMHeadModel(\n","    (transformer): GPT2Model(\n","      (wte): Embedding(50000, 768)\n","      (wpe): Embedding(1024, 768)\n","      (drop): Dropout(p=0.1, inplace=False)\n","      (h): ModuleList(\n","        (0): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): Attention(\n","            (c_attn): Conv1D()\n","            (c_proj): Conv1D()\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Conv1D()\n","            (c_proj): Conv1D()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"919A8jO9FLvQ","executionInfo":{"status":"error","timestamp":1603947209729,"user_tz":-540,"elapsed":1529,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"3bc44b61-16f2-45d1-e8b5-18f0337c0548","colab":{"base_uri":"https://localhost:8080/","height":392}},"source":["loss_fct = torch.nn.CrossEntropyLoss(ignore_index=3)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learninig_rate)\n","\n","losses =[]\n","for epoch in range(n_epoch):\n","    count = 0\n","    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n","        for i, data in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            data = torch.stack(data)  # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n","            data = data.transpose(1, 0)\n","            data= data.to(ctx)\n","\n","            outputs = model(data, labels=data)\n","            _, logits = outputs[:2]\n","\n","            # Shift so that tokens < n predict n\n","            shift_logits = logits[..., :-1, :].contiguous()\n","            shift_labels = data[..., 1:].contiguous()\n","\n","            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            losses.append(loss.item())\n","\n","            # if count % 10 == 0:\n","            #     print('epoch no.{} train no.{}  loss = {}'.format(epoch, count + 1, loss))\n","            if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n","                torch.save({\n","                    'epoch': epoch,\n","                    'train_no': count,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': loss}, save_ckpt_path)\n","            \n","            count += 1\n","            pbar.update(1)\n","            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train(0):   0%|          | 0/12531 [00:00<?, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-521d3edeb232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/RogerHeederer/ChatBot/KoGPT2_Wellness/model/kogpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             )\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         )\n\u001b[1;32m    247\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 15.75 GiB total capacity; 14.38 GiB already allocated; 38.88 MiB free; 14.44 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"yPq-NEfIPBLF"},"source":[""],"execution_count":null,"outputs":[]}]}